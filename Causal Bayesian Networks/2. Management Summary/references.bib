%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jonas Gottal at 2022-06-24 18:41:10 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Friedman1999,
	author = {Friedman, Nir and Goldszmidt, Moises and Wyner, Abraham},
	booktitle = {UAI '99: Proceedings of the 15th Annual Conference on Uncertainty in Artificial Intelligence},
	date-added = {2022-06-24 18:41:03 +0200},
	date-modified = {2022-06-24 18:41:03 +0200},
	pages = {196 -- 205},
	publisher = {Morgan Kaufmann},
	title = {Data Analysis with Bayesian Networks: A Bootstrap Approach},
	year = {1999}}

@article{Imoto2002,
	author = {Imoto, Seiya and Kim, S and Shimodaira, Hidetoshi and Aburatani, Sachiyo and Tashiro, K and Kuhara, S and Miyano, Satoru},
	date-added = {2022-06-24 18:39:07 +0200},
	date-modified = {2022-06-24 18:39:07 +0200},
	doi = {10.11234/gi1990.13.369},
	journal = {Genome Informatics},
	pages = {369 -- 370},
	title = {Bootstrap Analysis of Gene Networks Based on Bayesian Networks and Nonparametric Regression},
	volume = {13},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.11234/gi1990.13.369}}

@article{Scutari2019b,
	abstract = {Learning the structure of Bayesian networks from data is known to be a computationally challenging, NP-hard problem. The literature has long investigated how to perform structure learning from data containing large numbers of variables, following a general interest in high-dimensional applications (``small n, large p'') in systems biology and genetics. More recently, data sets with large numbers of observations (the so-called ``big data'') have become increasingly common; and these data sets are not necessarily high-dimensional, sometimes having only a few tens of variables depending on the application. We revisit the computational complexity of Bayesian network structure learning in this setting, showing that the common choice of measuring it with the number of estimated local distributions leads to unrealistic time complexity estimates for the most common class of score-based algorithms, greedy search. We then derive more accurate expressions under common distributional assumptions. These expressions suggest that the speed of Bayesian network learning can be improved by taking advantage of the availability of closed-form estimators for local distributions with few parents. Furthermore, we find that using predictive instead of in-sample goodness-of-fit scores improves speed; and we confirm that it improves the accuracy of network reconstruction as well, as previously observed by Chickering and Heckerman (Stat Comput 10: 55--62, 2000). We demonstrate these results on large real-world environmental and epidemiological data; and on reference data sets available from public repositories.},
	author = {Scutari, Marco and Vitolo, Claudia and Tucker, Allan},
	date = {2019/09/01},
	date-added = {2022-06-20 14:37:09 +0200},
	date-modified = {2022-06-20 14:37:09 +0200},
	doi = {10.1007/s11222-019-09857-1},
	id = {Scutari2019},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {5},
	pages = {1095 -- 1108},
	title = {Learning Bayesian networks from big data with greedy search: computational complexity and efficient implementation},
	url = {https://doi.org/10.1007/s11222-019-09857-1},
	volume = {29},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-019-09857-1}}

@article{Castelletti2018,
	author = {Castelletti, Federico and Consonni, Guido and Vedova, Marco L. Della and Peluso, Stefano},
	date-added = {2022-06-13 09:28:11 +0200},
	date-modified = {2022-06-13 09:28:17 +0200},
	doi = {10.1214/18-BA1101},
	journal = {Bayesian Analysis},
	number = {4},
	pages = {1235 -- 1260},
	publisher = {International Society for Bayesian Analysis},
	title = {{Learning Markov Equivalence Classes of Directed Acyclic Graphs: An Objective Bayes Approach}},
	url = {https://doi.org/10.1214/18-BA1101},
	volume = {13},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1214/18-BA1101}}

@inproceedings{Druzdzel2009,
	abstract = {The paper looks at the conditional independence search approach to causal discovery, proposed by Spirtes et al. and Pearl and Verma, from the point of view of the mechanism-based view of causality in econometrics, explicated by Simon. As demonstrated by Simon, the problem of determining the causal structure from data is severely underconstrained and the perceived causal structure depends on the a priori assumptions that one is willing to make. I discuss the assumptions made in the independence search-based causal discovery and their identifying strength.},
	author = {Druzdzel, Marek J.},
	booktitle = {8th Workshop on Uncertainty Processing (WUPES-09)},
	date-added = {2022-06-12 19:52:19 +0200},
	date-modified = {2022-06-12 19:53:15 +0200},
	month = {September},
	pages = {57 -- 68},
	title = {The Role of Assumptions in Causal Discovery},
	url = {http://d-scholarship.pitt.edu/6017/},
	year = {2009},
	bdsk-url-1 = {http://d-scholarship.pitt.edu/6017/}}

@article{Spirtes1991,
	abstract = { Previous asymptotically correct algorithms for recovering causal structure from sample probabilities have been limited even in sparse causal graphs to a few variables. We describe an asymptotically correct algorithm whose complexity for fixed graph connectivity increases polynomially in the number of vertices, and may in practice recover sparse graphs with several hundred variables. From sample data with n = 20,000, an implementation of the algorithm on a DECStation 3100 recovers the edges in a linear version of the ALARM network with 37 vertices and 46 edges. Fewer than 8\% of the undirected edges are incorrectly identified in the output. Without prior ordering information, the program also determines the direction of edges for the ALARM graph with an error rate of 14\%. Processing time is less than 10 seconds. Keywords DAGS, Causal Modelling. },
	author = {Spirtes, Peter and Glymour, Clark},
	date-added = {2022-06-12 18:58:34 +0200},
	date-modified = {2022-06-12 18:58:34 +0200},
	doi = {10.1177/089443939100900106},
	eprint = {https://doi.org/10.1177/089443939100900106},
	journal = {Social Science Computer Review},
	number = {1},
	pages = {62 -- 72},
	title = {An Algorithm for Fast Recovery of Sparse Causal Graphs},
	url = {https://doi.org/10.1177/089443939100900106},
	volume = {9},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1177/089443939100900106}}

@article{Graczyk2016,
	abstract = {We study the intraday behaviour of the statistical moments of the trading volume of the blue chip equities that composed the Dow Jones Industrial Average index between 2003 and 2014. By splitting that time interval into semesters, we provide a quantitative account of the nonstationary nature of the intraday statistical properties as well. Explicitly, we prove the well-known ∪-shape exhibited by the average trading volume---as well as the volatility of the price fluctuations---experienced a significant change from 2008 (the year of the ``subprime'' financial crisis) onwards. That has resulted in a faster relaxation after the market opening and relates to a consistent decrease in the convexity of the average trading volume intraday profile. Simultaneously, the last part of the session has become steeper as well, a modification that is likely to have been triggered by the new short-selling rules that were introduced in 2007 by the Securities and Exchange Commission. The combination of both results reveals that the ∪ has been turning into a ⊔. Additionally, the analysis of higher-order cumulants---namely the skewness and the kurtosis---shows that the morning and the afternoon parts of the trading session are each clearly associated with different statistical features and hence dynamical rules. Concretely, we claim that the large initial trading volume is due to wayward stocks whereas the large volume during the last part of the session hinges on a cohesive increase of the trading volume. That dissimilarity between the two parts of the trading session is stressed in periods of higher uproar in the market.},
	author = {Graczyk, Michelle B. AND Duarte Queir{\'o}s, S{\'\i}lvio M.},
	date-added = {2022-06-12 10:54:34 +0200},
	date-modified = {2022-06-12 10:54:53 +0200},
	doi = {10.1371/journal.pone.0165057},
	journal = {PLOS ONE},
	month = {11},
	number = {11},
	pages = {1 -- 25},
	publisher = {Public Library of Science},
	title = {Intraday Seasonalities and Nonstationarity of Trading Volume in Financial Markets: Individual and Cross-Sectional Features},
	url = {https://doi.org/10.1371/journal.pone.0165057},
	volume = {11},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0165057}}

@article{Alexander2015,
	author = {Alexander, Carol and Kapraun, Julia and Korovilas, Dimitris},
	date-added = {2022-06-11 15:02:51 +0200},
	date-modified = {2022-06-11 15:07:35 +0200},
	doi = {10.1111/fmii.12032},
	journal = {Financial Markets, Institutions \& Instruments},
	month = {11},
	title = {Trading and Investing in Volatility Products},
	volume = {24},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1111/fmii.12032}}

@article{Fassas2021,
	abstract = {This study tests and documents the information content of all publicly available implied volatility indices regarding both the realized volatility and the returns of the underlying asset. These topics present a path traveled by earlier work, but there are gains in studying together all 47 volatility-based indices that are now available, in order to examine if different asset classes and financial instruments could possess different return-volatility relations and forecasting ability.},
	author = {Fassas, Athanasios and Siriopoulos, Costas},
	date-added = {2022-06-11 14:59:04 +0200},
	date-modified = {2022-06-11 15:01:32 +0200},
	journal = {The Quarterly Review of Economics and Finance},
	number = {C},
	pages = {303-329},
	title = {Implied volatility indices -- A review},
	url = {https://EconPapers.repec.org/RePEc:eee:quaeco:v:79:y:2021:i:c:p:303-329},
	volume = {79},
	year = {2021},
	bdsk-url-1 = {https://EconPapers.repec.org/RePEc:eee:quaeco:v:79:y:2021:i:c:p:303-329}}

@misc{BlackRock2022,
	author = {BlackRock},
	date-added = {2022-06-11 13:42:25 +0200},
	date-modified = {2022-06-11 13:52:31 +0200},
	howpublished = {\url{https://www.ishares.com/us/literature/fact-sheet/fxi-ishares-china-large-cap-etf-fund-fact-sheet-en-us.pdf}},
	institution = {BlackRock},
	lastchecked = {11.06.2022},
	month = {March},
	note = {[Online; accessed 11-June-2022]},
	title = {Fact Sheet as of 03/31/2022 -- {iShares China Large-Cap ETF}},
	url = {https://www.ishares.com/us/literature/fact-sheet/fxi-ishares-china-large-cap-etf-fund-fact-sheet-en-us.pdf},
	year = {2022},
	bdsk-url-1 = {https://www.ishares.com/us/literature/fact-sheet/fxi-ishares-china-large-cap-etf-fund-fact-sheet-en-us.pdf}}

@book{Joshi2008,
	author = {Joshi, Mark S.},
	date-added = {2022-06-11 09:04:10 +0200},
	date-modified = {2022-06-11 09:05:33 +0200},
	edition = {2nd},
	publisher = {Cambridge University Press},
	series = {Mathematics, Finance and Risk},
	title = {The Concepts and Practice of Mathematical Finance},
	year = {2008}}

@techreport{Carvalho2009,
	author = {Carvalho, Alexandra M.},
	date-added = {2022-06-09 09:16:53 +0200},
	date-modified = {2022-06-09 09:31:30 +0200},
	institution = {Instituto Superior T{\'e}cnico, Technical University of Lisbon},
	journal = {INESC-ID Tec. Rep. 54/2009},
	title = {Scoring functions for learning Bayesian networks},
	year = {2009}}

@book{Shumway2017,
	author = {Shumway, Robert H. and Stoffer, David S.},
	date-added = {2022-06-08 20:54:01 +0200},
	date-modified = {2022-06-08 20:55:29 +0200},
	doi = {10.1007/978-3-319-52452-8},
	edition = {4th},
	publisher = {Springer International Publishing},
	title = {Time Series Analysis and Its Applications},
	url = {https://doi.org/10.1007/978-3-319-52452-8},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-52452-8}}

@article{Glymour2019,
	abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
	author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
	date-added = {2022-05-19 16:43:12 +0200},
	date-modified = {2022-05-19 16:43:12 +0200},
	doi = {10.3389/fgene.2019.00524},
	issn = {1664-8021},
	journal = {Frontiers in Genetics},
	title = {Review of Causal Discovery Methods Based on Graphical Models},
	url = {https://www.frontiersin.org/article/10.3389/fgene.2019.00524},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {https://www.frontiersin.org/article/10.3389/fgene.2019.00524},
	bdsk-url-2 = {https://doi.org/10.3389/fgene.2019.00524}}

@article{Scutari2019,
	abstract = {Three classes of algorithms to learn the structure of Bayesian networks from data are common in the literature: constraint-based algorithms, which use conditional independence tests to learn the dependence structure of the data; score-based algorithms, which use goodness-of-fit scores as objective functions to maximise; and hybrid algorithms that combine both approaches. Constraint-based and score-based algorithms have been shown to learn the same structures when conditional independence and goodness of fit are both assessed using entropy and the topological ordering of the network is known [1]. In this paper, we investigate how these three classes of algorithms perform outside the assumptions above in terms of speed and accuracy of network reconstruction for both discrete and Gaussian Bayesian networks. We approach this question by recognising that structure learning is defined by the combination of a statistical criterion and an algorithm that determines how the criterion is applied to the data. Removing the confounding effect of different choices for the statistical criterion, we find using both simulated and real-world complex data that constraint-based algorithms are often less accurate than score-based algorithms, but are seldom faster (even at large sample sizes); and that hybrid algorithms are neither faster nor more accurate than constraint-based algorithms. This suggests that commonly held beliefs on structure learning in the literature are strongly influenced by the choice of particular statistical criteria rather than just by the properties of the algorithms themselves.},
	author = {Scutari, Marco and Graafland, Catharina Elisabeth and Guti{\'e}rrez, Jos{\'e} Manuel},
	date-added = {2022-05-19 16:21:23 +0200},
	date-modified = {2022-06-04 14:20:47 +0200},
	doi = {https://doi.org/10.1016/j.ijar.2019.10.003},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	pages = {235--253},
	title = {Who learns better Bayesian network structures: Accuracy and speed of structure learning algorithms},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X19301434},
	volume = {115},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0888613X19301434},
	bdsk-url-2 = {https://doi.org/10.1016/j.ijar.2019.10.003}}

@article{Chicharro2014,
	abstract = {In recent years, powerful general algorithms of causal inference have been developed. In particular, in the framework of Pearl's causality, algorithms of inductive causation (IC and IC<sup>*</sup>) provide a procedure to determine which causal connections among nodes in a network can be inferred from empirical observations even in the presence of latent variables, indicating the limits of what can be learned without active manipulation of the system. These algorithms can in principle become important complements to established techniques such as Granger causality and Dynamic Causal Modeling (DCM) to analyze causal influences (effective connectivity) among brain regions. However, their application to dynamic processes has not been yet examined. Here we study how to apply these algorithms to time-varying signals such as electrophysiological or neuroimaging signals. We propose a new algorithm which combines the basic principles of the previous algorithms with Granger causality to obtain a representation of the causal relations suited to dynamic processes. Furthermore, we use graphical criteria to predict dynamic statistical dependencies between the signals from the causal structure. We show how some problems for causal inference from neural signals (e.g., measurement noise, hemodynamic responses, and time aggregation) can be understood in a general graphical approach. Focusing on the effect of spatial aggregation, we show that when causal inference is performed at a coarser scale than the one at which the neural sources interact, results strongly depend on the degree of integration of the neural sources aggregated in the signals, and thus characterize more the intra-areal properties than the interactions among regions. We finally discuss how the explicit consideration of latent processes contributes to understand Granger causality and DCM as well as to distinguish functional and effective connectivity.},
	author = {Chicharro, Daniel and Panzeri, Stefano},
	date-added = {2022-05-19 16:16:28 +0200},
	date-modified = {2022-05-19 16:16:28 +0200},
	doi = {10.3389/fninf.2014.00064},
	issn = {1662-5196},
	journal = {Frontiers in Neuroinformatics},
	title = {Algorithms of causal inference for the analysis of effective connectivity among brain regions},
	url = {https://www.frontiersin.org/article/10.3389/fninf.2014.00064},
	volume = {8},
	year = {2014},
	bdsk-url-1 = {https://www.frontiersin.org/article/10.3389/fninf.2014.00064},
	bdsk-url-2 = {https://doi.org/10.3389/fninf.2014.00064}}

@inproceedings{Pearl1991,
	address = {San Francisco, CA, USA},
	author = {Pearl, Judea and Verma, Thomas},
	booktitle = {Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning},
	date-added = {2022-05-19 16:14:18 +0200},
	date-modified = {2022-05-19 16:14:40 +0200},
	isbn = {1558601651},
	location = {Cambridge, MA, USA},
	numpages = {12},
	pages = {441--452},
	publisher = {Morgan Kaufmann Publishers Inc.},
	series = {KR'91},
	title = {A Theory of Inferred Causation},
	year = {1991}}

@book{Pearl2009,
	address = {Cambridge, UK},
	author = {Pearl, Judea},
	date-added = {2022-05-19 16:11:05 +0200},
	date-modified = {2022-05-19 16:12:27 +0200},
	doi = {10.1017/CBO9780511803161},
	edition = {2nd},
	isbn = {978-0-521-89560-6},
	publisher = {Cambridge University Press},
	subtitle = {Models, Reasoning, and Inference},
	title = {Causality},
	year = 2009,
	bdsk-url-1 = {https://doi.org/10.1017/CBO9780511803161}}

@inproceedings{Meek1995,
	abstract = {This paper presents correct algorithms for answering the following two questions; (i) Does there exist a causal explanation consistent with a set of background knowledge which explains all of the observed independence facts in a sample? (ii) Given that there is such a causal explanation what are the causal relationships common to every such causal explanation?},
	address = {San Francisco, CA, USA},
	author = {Meek, Christopher},
	booktitle = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
	date-added = {2022-05-19 16:08:59 +0200},
	date-modified = {2022-05-19 16:09:31 +0200},
	isbn = {1558603859},
	location = {Montr\'{e}al, Qu\'{e}, Canada},
	numpages = {8},
	pages = {403--410},
	publisher = {Morgan Kaufmann Publishers Inc.},
	series = {UAI'95},
	title = {Causal Inference and Causal Explanation with Background Knowledge},
	year = {1995}}

@inproceedings{Borgelt1999,
	abstract = {In this paper we consider the problem of inducing causal relations from statistical data. Although it is well known that a correlation does not justify the claim of a causal relation between two measures, the question seems not to be settled. Research in the field of Bayesian networks revived an approach suggested in [16]. It is based on the idea that there are relationships between the causal structure of a domain and its corresponding probability distribution, which could be exploited to infer at least part of the causal structure from a set of dependence and independence statements. This idea was developed into the inductive causation algorithm [14]. We review this algorithm and examine the assumptions underlying it.},
	address = {Berlin, Heidelberg},
	author = {Borgelt, Christian and Kruse, Rudolf},
	booktitle = {Symbolic and Quantitative Approaches to Reasoning and Uncertainty},
	date-added = {2022-05-19 16:06:01 +0200},
	date-modified = {2022-05-19 16:06:29 +0200},
	editor = {Hunter, Anthony and Parsons, Simon},
	isbn = {978-3-540-48747-0},
	pages = {68--79},
	publisher = {Springer Berlin Heidelberg},
	title = {A Critique of Inductive Causation},
	year = {1999}}

@book{Hull2021,
	address = {Boston},
	author = {Hull, John C.},
	date-added = {2022-05-19 15:58:51 +0200},
	date-modified = {2022-05-19 16:00:32 +0200},
	edition = {11th},
	publisher = {Pearson},
	title = {Options, Futures, and other Derivatives},
	year = {2021}}

@misc{Wikipedia2022,
	author = {{Wikipedia contributors}},
	date-added = {2022-02-13 19:03:06 +0100},
	date-modified = {2022-02-13 19:07:29 +0100},
	howpublished = {\url{https://en.wikipedia.org/w/index.php?title=Duck_test&oldid=1069942509}},
	note = {[Online; accessed 13-February-2022]},
	title = {Duck test -- {Wikipedia}{,} The Free Encyclopedia},
	year = {2022}}

@book{Kauermann2021,
	author = {Kauermann, G\"{o}ran and K\"{u}chenhoff, Helmut and Heumann, Christian},
	date-added = {2022-01-17 18:47:05 +0100},
	date-modified = {2022-01-17 18:47:58 +0100},
	doi = {10.1007/978-3-030-69827-0},
	publisher = {Springer International Publishing},
	title = {Statistical Foundations, Reasoning and Inference},
	url = {https://doi.org/10.1007/978-3-030-69827-0},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-69827-0}}

@article{Stigler2013,
	author = {Stigler, Stephen M.},
	date-added = {2022-01-06 14:49:24 +0100},
	date-modified = {2022-01-06 14:50:35 +0100},
	doi = {10.1214/13-sts438},
	issn = {0883-4237},
	journal = {Statistical Science},
	number = {3},
	publisher = {Institute of Mathematical Statistics},
	title = {The True Title of Bayes's Essay},
	url = {http://dx.doi.org/10.1214/13-STS438},
	volume = {28},
	year = {2013},
	bdsk-url-1 = {http://dx.doi.org/10.1214/13-STS438},
	bdsk-url-2 = {http://dx.doi.org/10.1214/13-sts438}}

@book{Hastie2009,
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	date-added = {2022-01-02 13:52:43 +0100},
	date-modified = {2022-02-13 19:44:55 +0100},
	doi = {10.1007/978-0-387-84858-7},
	edition = {2nd},
	publisher = {Springer New York},
	title = {The Elements of Statistical Learning},
	url = {https://doi.org/10.1007/978-0-387-84858-7},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1007/978-0-387-84858-7}}

@article{Schwarz1978,
	author = {Schwarz, Gideon},
	date-added = {2022-01-02 13:41:39 +0100},
	date-modified = {2022-02-13 19:39:57 +0100},
	doi = {10.1214/aos/1176344136},
	journal = {The Annals of Statistics},
	number = {2},
	pages = {461--464},
	publisher = {Institute of Mathematical Statistics},
	title = {Estimating the Dimension of a Model},
	url = {https://doi.org/10.1214/aos/1176344136},
	volume = {6},
	year = {1978},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176344136}}

@article{Neath2012,
	abstract = {The Bayesian information criterion (BIC) is one of the most widely known and pervasively used tools in statistical model selection. Its popularity is derived from its computational simplicity and effective performance in many modeling frameworks, including Bayesian applications where prior distributions may be elusive. The criterion was derived by Schwarz (Ann Stat 1978, 6:461--464) to serve as an asymptotic approximation to a transformation of the Bayesian posterior probability of a candidate model. This article reviews the conceptual and theoretical foundations for BIC, and also discusses its properties and applications. WIREs Comput Stat 2012, 4:199--203. doi: 10.1002/wics.199This article is categorized under:
1Statistical and Graphical Methods of Data Analysis &gt; Bayesian Methods and Theory2Statistical and Graphical Methods of Data Analysis &gt; Information Theoretic Methods3Statistical Learning and Exploratory Methods of the Data Sciences &gt; Modeling Methods},
	address = {USA},
	author = {Neath, Andrew A. and Cavanaugh, Joseph E.},
	date-added = {2022-01-02 13:39:58 +0100},
	date-modified = {2022-02-13 19:46:08 +0100},
	doi = {10.1002/wics.199},
	issn = {1939-5108},
	issue_date = {March/April 2012},
	journal = {WIREs Computational Statistics},
	number = {2},
	numpages = {5},
	pages = {199--203},
	publisher = {John Wiley &amp; Sons, Inc.},
	title = {The Bayesian Information Criterion: Background, Derivation, and Applications},
	url = {https://doi.org/10.1002/wics.199},
	volume = {4},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1002/wics.199}}

@incollection{Holmes2008,
	abstract = {Reasoning with incomplete and unreliable information is a central characteristic of decision making, for example in industry, medicine and finance. Bayesian networks provide a theoretical framework for dealing with this uncertainty using an underlying graphical structure and the probability calculus. Bayesian networks have been successfully implemented in areas as diverse as medical diagnosis and finance. We present a brief introduction to Bayesian networks for those readers new to them and give some pointers to the literature.},
	address = {Berlin, Heidelberg},
	author = {Holmes, Dawn E. and Jain, Lakhmi C.},
	booktitle = {Innovations in Bayesian Networks: Theory and Applications},
	date-added = {2021-12-04 12:04:16 +0100},
	date-modified = {2021-12-04 12:04:20 +0100},
	doi = {10.1007/978-3-540-85066-3_1},
	editor = {Holmes, Dawn E. and Jain, Lakhmi C.},
	isbn = {978-3-540-85066-3},
	pages = {1--5},
	publisher = {Springer Berlin Heidelberg},
	title = {Introduction to Bayesian Networks},
	url = {https://doi.org/10.1007/978-3-540-85066-3_1},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-540-85066-3_1}}

@article{Heckerman1995,
	abstract = {We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen---a prior network---and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k = 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.},
	author = {Heckerman, David AND Geiger, Dan AND Chickering, David M.},
	da = {1995/09/01},
	date-added = {2021-12-04 11:52:19 +0100},
	date-modified = {2021-12-04 11:54:26 +0100},
	doi = {10.1023/A:1022623210503},
	id = {Heckerman1995},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {197--243},
	title = {Learning Bayesian Networks: The Combination of Knowledge and Statistical Data},
	ty = {JOUR},
	url = {https://doi.org/10.1023/A:1022623210503},
	volume = {20},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1023/A:1022623210503}}

@incollection{Heckerman2008,
	abstract = {A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with Bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.},
	address = {Berlin, Heidelberg},
	author = {Heckerman, David},
	booktitle = {Innovations in Bayesian Networks: Theory and Applications},
	date-added = {2021-12-04 11:50:30 +0100},
	date-modified = {2021-12-04 11:59:48 +0100},
	doi = {10.1007/978-3-540-85066-3_3},
	editor = {Holmes, Dawn E. and Jain, Lakhmi C.},
	isbn = {978-3-540-85066-3},
	pages = {33--82},
	publisher = {Springer Berlin Heidelberg},
	title = {A Tutorial on Learning with Bayesian Networks},
	url = {https://doi.org/10.1007/978-3-540-85066-3_3},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-540-85066-3_3}}

@inproceedings{Friedman1998,
	abstract = {In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data--that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof.},
	address = {San Francisco, CA},
	author = {Friedman, Nir},
	booktitle = {Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence},
	date-added = {2021-12-04 11:47:46 +0100},
	date-modified = {2022-06-07 20:45:41 +0200},
	isbn = {155860555X},
	location = {Madison, Wisconsin},
	numpages = {10},
	pages = {129--138},
	publisher = {Morgan Kaufmann Publishers Inc.},
	title = {The Bayesian Structural {EM} Algorithm},
	year = {1998}}

@book{Bernardo2000,
	author = {Bernardo, Jos{\'e} M. and Smith, Adrian F. M.},
	date-added = {2021-12-04 11:19:39 +0100},
	date-modified = {2022-02-13 19:37:37 +0100},
	publisher = {John Wiley \& Sons, LTD},
	title = {Bayesian Theory},
	year = {2000}}

@article{Dempster1977,
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
	date-added = {2021-12-04 11:13:16 +0100},
	date-modified = {2021-12-04 11:13:27 +0100},
	issn = {00359246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {1--38},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
	url = {http://www.jstor.org/stable/2984875},
	volume = {39},
	year = {1977},
	bdsk-url-1 = {http://www.jstor.org/stable/2984875}}

@article{Buntine1996,
	author = {Buntine, Wray},
	date-added = {2021-12-04 11:09:09 +0100},
	date-modified = {2022-02-13 19:39:43 +0100},
	doi = {10.1109/69.494161},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	pages = {195--210},
	title = {A Guide to the Literature on Learning Probabilistic Networks from Data},
	volume = {8},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/69.494161}}

@article{Lauritzen1988,
	abstract = {A causal network is used in a number of areas as a depiction of patterns of `influence' among sets of variables. In expert systems it is common to perform `inference' by means of local computations on such large but sparse networks. In general, non-probabilistic methods are used to handle uncertainty when propagating the effects of evidence, and it has appeared that exact probabilistic methods are not computationally feasible. Motivated by an application in electromyography, we counter this claim by exploiting a range of local representations for the joint probability distribution, combined with topological changes to the original network termed `marrying' and `filling-in'. The resulting structure allows efficient algorithms for transfer between representations, providing rapid absorption and propagation of evidence. The scheme is first illustrated on a small, fictitious but challenging example, and the underlying theory and computational aspects are then discussed.},
	author = {S. L. Lauritzen and D. J. Spiegelhalter},
	date-added = {2021-12-04 10:56:14 +0100},
	date-modified = {2021-12-04 10:56:33 +0100},
	issn = {00359246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {2},
	pages = {157--224},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Local Computations with Probabilities on Graphical Structures and Their Application to Expert Systems},
	url = {http://www.jstor.org/stable/2345762},
	volume = {50},
	year = {1988},
	bdsk-url-1 = {http://www.jstor.org/stable/2345762}}

@techreport{Scutari2022,
	author = {Scutari, Marco},
	date-added = {2021-12-03 17:49:53 +0100},
	date-modified = {2022-06-10 11:00:08 +0200},
	institution = {CRAN},
	title = {Package `bnlearn'},
	url = {https://cran.r-project.org/web/packages/bnlearn/bnlearn.pdf},
	urldate = {31.03.2022},
	year = {2022},
	bdsk-url-1 = {https://cran.r-project.org/web/packages/bnlearn/bnlearn.pdf}}

@book{Ertel2020,
	author = {Ertel, Wolfgang},
	date-added = {2021-12-03 17:46:07 +0100},
	date-modified = {2021-12-03 17:46:47 +0100},
	edition = {5th},
	publisher = {Springer-Verlag},
	title = {Grundkurs K{\"u}nstliche Intelligenz - Eine praxisorientierte Einf{\"u}hrung},
	year = {2020}}

@book{Nagarajan2014,
	author = {Nagarajan, Radhakrishnan AND Scutari, Marco AND L{\`e}bre, Sophie},
	date-added = {2021-12-03 17:45:03 +0100},
	date-modified = {2022-01-02 13:59:51 +0100},
	doi = {10.1007/978-1-4614-6446-4},
	publisher = {Springer New York},
	title = {Bayesian Networks in R},
	url = {https://doi.org/10.1007/978-1-4614-6446-4},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4614-6446-4}}

@book{Mitchell1997,
	address = {New York},
	author = {Mitchell, Tom Michael},
	date-added = {2021-12-03 17:44:18 +0100},
	date-modified = {2021-12-03 17:44:40 +0100},
	publisher = {McGraw-Hill},
	title = {Machine Learning},
	year = {1997}}

@book{Russell2021,
	author = {Russell, Stuart Jonathan AND Norvig, Peter},
	date-added = {2021-12-03 17:39:55 +0100},
	date-modified = {2022-01-02 11:17:20 +0100},
	edition = {4th},
	publisher = {Pearson},
	title = {Artificial Intelligence -- A Modern Approach},
	year = {2021}}
